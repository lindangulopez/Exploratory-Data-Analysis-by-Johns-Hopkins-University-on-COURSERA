---
title: "Grouping Data in R"
author: "Linda Angulo Lopez"
date: "19/09/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Principal Components Analysis  and Singular Value Decomposition:

Are useful for both data exploration and in the final modeling phase. 
- data set with no apparent pattern

```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```


```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```
- data set with a pattern

```{r}
set.seed(678910)
for (i in 1:40) {
 # flip a coin
 coinFlip <- rbinom(1, size = 1, prob = 0.5)
 # if coin is heads add a common pattern to that row
 if (coinFlip) {
 dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
 }
}

```

```{r}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

```


```{r}

hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, , xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", ylab = "Column Mean", pch = 19)
```

###  Components of the SVD - Components of the SVD - u and v
- Variance:

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, , xlab = "Row", ylab = "First left singular vector",
 pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)

```
- variance explained

```{r}
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```
### Relationship of SVD to PCA:

```{r}

svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab = "Principal Component 1",
 ylab = "Right Singular Vector 1")
abline(c(0, 1))

```
### Example with 0 1 matrix:

```{r}
constantMatrix <- dataMatrixOrdered*0
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0,1),each=5)}
svd1 <- svd(constantMatrix)
par(mfrow=c(1,3))
image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Prop. of variance explained",pch=19)
```
## SVD, True Patterns: 
```{r}
set.seed(678910)
for (i in 1:40) {
 # flip a coin
 coinFlip1 <- rbinom(1, size = 1, prob = 0.5)
 coinFlip2 <- rbinom(1, size = 1, prob = 0.5)
 # if coin is heads add a common pattern to that row
 if (coinFlip1) {
 dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5), each = 5)
 }
 if (coinFlip2) {
 dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5), 5)
 }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]

```


```{r}

svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each = 5), pch = 19, xlab = "Column", ylab = "Pattern 1")
plot(rep(c(0, 1), 5), pch = 19, xlab = "Column", ylab = "Pattern 2")

```
 - v and patterns of variance in rows and patterns of variance in rows:
```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "Column", ylab = "Second right singular vector")

```
- d and variance explained

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance explained",
 pch = 19)

```
### Dealing with missing values:

```{r}
dataMatrix2 <- dataMatrixOrdered
## Randomly insert some missing data
dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
#svd1 <- svd(scale(dataMatrix2)) ## Doesn't work!
#Error in svd(scale(dataMatrix2)) : infinite or missing values in 'x'
summary(dataMatrix2)
```

```{r}
library(mice)
md.pattern(dataMatrix2)
```

```{r}
library(VIM)
mice_plot <- aggr(dataMatrix2, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(dataMatrix2), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))

```

```{r}
imputed_Data <- mice(dataMatrix2, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imputed_Data)
```


```{r}
mice(data = dataMatrix2, m = 5, method = "pmm", maxit = 50, seed = 500)

```


```{r}
summary(dataMatrix2)
```


```{r}
summary(imputed_Data)
```


```{r}
completeData <- complete(imputed_Data,1)
```



```{r}
summary(completeData)
```



```{r}

svd1 <- svd(scale(dataMatrixOrdered)); svd2 <- svd(scale(completeData))
par(mfrow=c(1,2)); plot(svd1$v[,1],pch=19); plot(svd2$v[,1],pch=19)
```
### Face Example:

```{r}
load("C:/Users/angul/OneDrive/R/ExploreData/Data/face.rda")
image(t(faceData)[, nrow(faceData):1])
```


```{r}
svd1 <- svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = "Singular vector", ylab = "Variance explained")
```


```{r}
svd1 <- svd(scale(faceData))
## Note that %*% is matrix multiplication
# Here svd1$d[1] is a constant
approx1 <- svd1$u[, 1] %*% t(svd1$v[, 1]) * svd1$d[1]
# In these examples we need to make the diagonal matrix out of d
approx5 <- svd1$u[, 1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[, 1:5])
approx10 <- svd1$u[, 1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[, 1:10])
```


```{r}
par(mfrow = c(2, 3))
image(t(approx1)[, nrow(approx1):1], main = "(a)")
image(t(approx5)[, nrow(approx5):1], main = "(b)")
image(t(approx10)[, nrow(approx10):1], main = "(c)")
image(t(faceData)[, nrow(faceData):1], main = "(d)") ## Original data

```


## K-means clustering:

Uses a partitioning approach by fixing “centroids” of each cluster then by iteration observations are assigned to the closest centroid. This can also be visualized with a heat map function or by plotting the points.

```{r}
set.seed(1234); par(mar=c(0,0,0,0))
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))

```


```{r}
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
names(kmeansObj)


```



```{r}
kmeansObj$cluster

```



```{r}
par(mar=rep(0.2,4))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```


```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <- kmeans(dataMatrix,centers=3)
par(mfrow=c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1],yaxt="n")
image(t(dataMatrix)[,order(kmeansObj2$cluster)],yaxt="n")
```


## Hierarchical clustering

Hierarchical clustering adopts an iterative merge of points by nearest distance,  to form clusters. Hence the  as the notion of distance is the most important basis for classification.

This type of analysis is good for exploratory analysis, as the picture may be unstable and is deterministic as you choose the cut of point and the distance chosen is the most important basis for classification. The global distance is usually defined as a weighted sum of the local distances, but it's better to experiment and inspect the results to see if it makes sense. 

```{r}

set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))

```


```{r}
dataFrame <- data.frame(x = x, y = y)
dist(dataFrame)

```

```{r}
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)

```

```{r}
library(rafalib)

dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
```

```{r}

myplclust <- function(hclust, lab = hclust$labels, lab.col = rep(1, length(hclust$labels)),
 hang = 0.1, ...) {
 ## modifiction of plclust for plotting hclust objects *in colour*! Copyright
 ## Eva KF Chan 2009 Arguments: hclust: hclust object lab: a character vector
 ## of labels of the leaves of the tree lab.col: colour for the labels;
 ## NA=default device foreground colour hang: as in hclust & plclust Side
 ## effect: A display of hierarchical cluster with coloured leaf labels.
 y <- rep(hclust$height, 2)
 x <- as.numeric(hclust$merge)
 y <- y[which(x < 0)]
 x <- x[which(x < 0)]
 x <- abs(x)
 y <- y[order(x)]
 x <- x[order(x)]
 plot(hclust, labels = FALSE, hang = hang, ...)
 text(x = x, y = y[hclust$order] - (max(hclust$height) * hang), labels = lab[hclust$order],
 col = lab.col[hclust$order], srt = 90, adj = c(1, 0.5), xpd = NA, ...)
}
```

```{r}
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
```

```{r}
dataFrame <- data.frame(x = x, y = y )
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
heatmap(dataMatrix)

```



[learn more about distance:](https://youtu.be/wQhVWUcXM0A), and see [myCOURSENOTES](https://www.linkedin.com/pulse/grouping-data-exploration-r-linda-angulo-l%C3%B3pez/)

